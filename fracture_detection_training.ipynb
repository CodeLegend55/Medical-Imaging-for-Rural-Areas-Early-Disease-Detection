{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f5805b99",
      "metadata": {
        "id": "f5805b99"
      },
      "source": [
        "# Fracture Detection Model Training with PyTorch\n",
        "\n",
        "This notebook trains three different deep learning models (ResNet50, DenseNet121, EfficientNetB0) for fracture detection using X-ray images. The models are trained on the Kaggle fracture multi-region X-ray dataset.\n",
        "\n",
        "**Dataset:** [Fracture Multi-Region X-ray Data](https://www.kaggle.com/datasets/bmadushanirodrigo/fracture-multi-region-x-ray-data/data)\n",
        "\n",
        "**Models:**\n",
        "- ResNet50\n",
        "- DenseNet121  \n",
        "- EfficientNetB0\n",
        "\n",
        "**Environment:** Google Colab with free GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd86b14d",
      "metadata": {
        "id": "cd86b14d"
      },
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6c8e35e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6c8e35e",
        "outputId": "9bf13bf1-41ae-4e93-d826-f6d537c7274b"
      },
      "outputs": [],
      "source": [
        "# Check if running on Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running on Google Colab\")\n",
        "    # Install required packages\n",
        "    !pip install kaggle\n",
        "    !pip install efficientnet-pytorch\n",
        "    !pip install albumentations\n",
        "else:\n",
        "    print(\"Not running on Colab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c086e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08c086e5",
        "outputId": "b877e310-b41a-4749-af87-daf016e4660c"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# For EfficientNet\n",
        "try:\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "except ImportError:\n",
        "    print(\"EfficientNet not installed, installing now...\")\n",
        "    !pip install efficientnet-pytorch\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d748e7b",
      "metadata": {
        "id": "0d748e7b"
      },
      "source": [
        "## 2. Kaggle API Setup and Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c00cc35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "3c00cc35",
        "outputId": "f4dc8be8-eab0-4f5a-a2a7-5fcb28238798"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive and setup Kaggle API\n",
        "if IN_COLAB:\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Create necessary directories in Drive\n",
        "    import os\n",
        "    drive_base = '/content/drive/MyDrive/Capstone'\n",
        "    models_dir = f'{drive_base}/Models'\n",
        "    dataset_dir = f'{drive_base}/Dataset'\n",
        "\n",
        "    os.makedirs(drive_base, exist_ok=True)\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Created directories:\")\n",
        "    print(f\"- Base: {drive_base}\")\n",
        "    print(f\"- Models: {models_dir}\")\n",
        "    print(f\"- Dataset: {dataset_dir}\")\n",
        "\n",
        "    # Setup Kaggle API\n",
        "    from google.colab import files\n",
        "    print(\"\\nPlease upload your kaggle.json file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Setup Kaggle API\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !mv kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    print(\"Kaggle API setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e53f995f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e53f995f",
        "outputId": "894b2001-44a2-433b-ccbe-18287bae8b78"
      },
      "outputs": [],
      "source": [
        "# Download the fracture dataset to Google Drive\n",
        "if IN_COLAB:\n",
        "    # Check if dataset already exists in Drive\n",
        "    dataset_zip_path = f'{dataset_dir}/fracture-multi-region-x-ray-data.zip'\n",
        "\n",
        "    if os.path.exists(dataset_zip_path):\n",
        "        print(\"Dataset already exists in Google Drive. Skipping download.\")\n",
        "    else:\n",
        "        print(\"Downloading dataset to Google Drive...\")\n",
        "        # Change to dataset directory\n",
        "        os.chdir(dataset_dir)\n",
        "\n",
        "        !kaggle datasets download -d bmadushanirodrigo/fracture-multi-region-x-ray-data\n",
        "        print(\"Dataset downloaded to Google Drive!\")\n",
        "\n",
        "    # Extract if not already extracted\n",
        "    extracted_check = os.path.join(dataset_dir, 'FractureDataset')\n",
        "    if not os.path.exists(extracted_check):\n",
        "        print(\"Extracting dataset...\")\n",
        "        os.chdir(dataset_dir)\n",
        "        !unzip -q fracture-multi-region-x-ray-data.zip\n",
        "        print(\"Dataset extracted!\")\n",
        "    else:\n",
        "        print(\"Dataset already extracted in Google Drive.\")\n",
        "\n",
        "    # Change back to content directory\n",
        "    os.chdir('/content')\n",
        "\n",
        "    # List the contents to understand the structure\n",
        "    print(\"\\nDataset structure in Google Drive:\")\n",
        "    for root, dirs, files in os.walk(dataset_dir):\n",
        "        level = root.replace(dataset_dir, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:  # Show only first 5 files\n",
        "            print(f\"{subindent}{file}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... and {len(files)-5} more files\")\n",
        "        if level > 3:  # Limit depth\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b979f92",
      "metadata": {
        "id": "6b979f92"
      },
      "source": [
        "## 3. Data Exploration and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "127fb1da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "127fb1da",
        "outputId": "f06ae580-6a2f-47e5-eb93-92b9d0eb637d"
      },
      "outputs": [],
      "source": [
        "# Define dataset paths (now using Google Drive)\n",
        "if IN_COLAB:\n",
        "    # Use Google Drive paths - check for all three folders\n",
        "    base_dataset_path = f'{dataset_dir}/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification'\n",
        "    train_dir = f'{base_dataset_path}/train'\n",
        "    val_dir = f'{base_dataset_path}/val'  # Use existing validation folder\n",
        "    test_dir = f'{base_dataset_path}/test'\n",
        "\n",
        "    # If the structure is different, explore and update\n",
        "    if not os.path.exists(train_dir):\n",
        "        # Find the actual dataset directory in Google Drive\n",
        "        print(\"Exploring dataset structure in Google Drive...\")\n",
        "        for root, dirs, files in os.walk(dataset_dir):\n",
        "            if 'train' in dirs or 'Fractured' in dirs or 'Non-Fractured' in dirs:\n",
        "                print(f\"Found potential dataset directory: {root}\")\n",
        "                print(f\"Subdirectories: {dirs}\")\n",
        "                # Update paths based on actual structure\n",
        "                if 'train' in dirs:\n",
        "                    base_dataset_path = root\n",
        "                    train_dir = os.path.join(root, 'train')\n",
        "                if 'val' in dirs:\n",
        "                    val_dir = os.path.join(root, 'val')\n",
        "                if 'test' in dirs:\n",
        "                    test_dir = os.path.join(root, 'test')\n",
        "                break\n",
        "else:\n",
        "    train_dir = './dataset/train'\n",
        "    val_dir = './dataset/val'\n",
        "    test_dir = './dataset/test'\n",
        "\n",
        "print(f\"Train directory: {train_dir}\")\n",
        "print(f\"Validation directory: {val_dir}\")\n",
        "print(f\"Test directory: {test_dir}\")\n",
        "print(f\"Models will be saved to: {models_dir if IN_COLAB else './models'}\")\n",
        "\n",
        "# Check which folders actually exist\n",
        "existing_folders = []\n",
        "if os.path.exists(train_dir):\n",
        "    existing_folders.append('train')\n",
        "if os.path.exists(val_dir):\n",
        "    existing_folders.append('val')\n",
        "if os.path.exists(test_dir):\n",
        "    existing_folders.append('test')\n",
        "\n",
        "print(f\"Available folders: {existing_folders}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "137175c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "137175c9",
        "outputId": "d436b9c3-dae4-4d2c-b0cb-5d7a70371351"
      },
      "outputs": [],
      "source": [
        "# Create a function to explore dataset structure and create file lists\n",
        "def explore_dataset_structure(base_path):\n",
        "    \"\"\"Explore and understand the dataset structure\"\"\"\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"Path {base_path} does not exist. Exploring available paths...\")\n",
        "\n",
        "        # Look for common fracture dataset patterns\n",
        "        search_path = dataset_dir if IN_COLAB else '.'\n",
        "        for root, dirs, files in os.walk(search_path):\n",
        "            if any(keyword in root.lower() for keyword in ['fracture', 'train', 'test']):\n",
        "                print(f\"Found: {root}\")\n",
        "                if dirs:\n",
        "                    print(f\"  Subdirs: {dirs}\")\n",
        "                if files[:3]:  # Show first 3 files\n",
        "                    print(f\"  Files: {files[:3]}...\")\n",
        "        return None, None\n",
        "\n",
        "    # If path exists, explore structure\n",
        "    class_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
        "    print(f\"Classes found in {base_path}: {class_dirs}\")\n",
        "\n",
        "    file_list = []\n",
        "    labels = []\n",
        "\n",
        "    # FIXED: Assign labels based on folder content, not alphabetical order\n",
        "    # Label 0 = Non-Fractured, Label 1 = Fractured (matching Flask app expectations)\n",
        "    for class_name in class_dirs:\n",
        "        class_path = os.path.join(base_path, class_name)\n",
        "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        print(f\"Class '{class_name}': {len(images)} images\")\n",
        "\n",
        "        class_name_lower = class_name.lower()\n",
        "        \n",
        "        # Determine correct label based on folder name\n",
        "        # Check for NON-fractured keywords FIRST (to handle \"not fractured\" correctly)\n",
        "        if any(keyword in class_name_lower for keyword in ['not fractured', 'non-fractured', 'normal', 'negative', 'no', '0']):\n",
        "            # This is the non-fractured class  \n",
        "            label = 0\n",
        "            print(f\"  -> Assigned as NON-FRACTURED (label=0)\")\n",
        "        elif any(keyword in class_name_lower for keyword in ['fractured', 'fracture', 'positive', 'yes', '1']):\n",
        "            # This is the fractured class\n",
        "            label = 1\n",
        "            print(f\"  -> Assigned as FRACTURED (label=1)\")\n",
        "        else:\n",
        "            # Fallback: ask user to verify or make educated guess\n",
        "            print(f\"  -> WARNING: Could not determine class type from name '{class_name}'\")\n",
        "            print(f\"     Please verify: Is '{class_name}' fractured (1) or non-fractured (0)?\")\n",
        "            # For now, assume alphabetical assignment but warn user\n",
        "            class_idx = class_dirs.index(class_name)\n",
        "            label = class_idx\n",
        "            print(f\"  -> Using alphabetical assignment: label={label}\")\n",
        "\n",
        "        for img in images:\n",
        "            file_list.append(os.path.join(class_path, img))\n",
        "            labels.append(label)\n",
        "\n",
        "    # Print summary\n",
        "    unique_labels = set(labels)\n",
        "    print(f\"\\nLabel assignment summary:\")\n",
        "    for lbl in sorted(unique_labels):\n",
        "        count = labels.count(lbl)\n",
        "        label_name = \"NON-FRACTURED\" if lbl == 0 else \"FRACTURED\"\n",
        "        print(f\"  Label {lbl} ({label_name}): {count} images\")\n",
        "\n",
        "    return file_list, labels\n",
        "\n",
        "# Explore the dataset from Google Drive\n",
        "search_location = train_dir if os.path.exists(train_dir) else (dataset_dir if IN_COLAB else '/content')\n",
        "train_files, train_labels = explore_dataset_structure(search_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91f2d264",
      "metadata": {
        "id": "91f2d264"
      },
      "outputs": [],
      "source": [
        "# Load data from all available folders (train, val, test)\n",
        "def load_dataset_from_folders():\n",
        "    \"\"\"Load dataset from existing train/val/test folders\"\"\"\n",
        "    all_files = []\n",
        "    all_labels = []\n",
        "    dataset_info = {}\n",
        "    \n",
        "    folders_to_check = [\n",
        "        ('train', train_dir),\n",
        "        ('val', val_dir), \n",
        "        ('test', test_dir)\n",
        "    ]\n",
        "    \n",
        "    for folder_name, folder_path in folders_to_check:\n",
        "        if os.path.exists(folder_path):\n",
        "            print(f\"\\n--- Loading {folder_name.upper()} data ---\")\n",
        "            files, labels = explore_dataset_structure(folder_path)\n",
        "            \n",
        "            if files:\n",
        "                dataset_info[folder_name] = {\n",
        "                    'files': files,\n",
        "                    'labels': labels,\n",
        "                    'count': len(files)\n",
        "                }\n",
        "                print(f\"Loaded {len(files)} images from {folder_name} folder\")\n",
        "                \n",
        "                # Add to combined dataset for overall statistics\n",
        "                all_files.extend(files)\n",
        "                all_labels.extend(labels)\n",
        "            else:\n",
        "                print(f\"No files found in {folder_name} folder\")\n",
        "        else:\n",
        "            print(f\"{folder_name.upper()} folder not found: {folder_path}\")\n",
        "    \n",
        "    return dataset_info, all_files, all_labels\n",
        "\n",
        "# Load from existing folder structure\n",
        "dataset_info, combined_files, combined_labels = load_dataset_from_folders()\n",
        "\n",
        "if combined_files:\n",
        "    print(f\"\\n=== DATASET SUMMARY ===\")\n",
        "    print(f\"Total images across all folders: {len(combined_files)}\")\n",
        "    total_label_counts = pd.Series(combined_labels).value_counts().sort_index()\n",
        "    print(f\"Overall label distribution:\")\n",
        "    for label, count in total_label_counts.items():\n",
        "        label_name = \"NON-FRACTURED\" if label == 0 else \"FRACTURED\"  \n",
        "        print(f\"  {label_name} (label={label}): {count} images\")\n",
        "    \n",
        "    # Print breakdown by folder\n",
        "    print(f\"\\nBreakdown by folder:\")\n",
        "    for folder_name, info in dataset_info.items():\n",
        "        folder_label_counts = pd.Series(info['labels']).value_counts().sort_index()\n",
        "        print(f\"  {folder_name.upper()}: {info['count']} images\")\n",
        "        for label, count in folder_label_counts.items():\n",
        "            label_name = \"NON-FRACTURED\" if label == 0 else \"FRACTURED\"\n",
        "            print(f\"    {label_name}: {count}\")\n",
        "else:\n",
        "    print(\"No images found in any folder. Please check the dataset structure.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "070118b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "070118b1",
        "outputId": "d0598489-0099-4dda-f168-f896b00b2599"
      },
      "outputs": [],
      "source": [
        "# Visualize sample images with CORRECTED labels\n",
        "if train_files:\n",
        "    # Sample some images to display\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
        "    fig.suptitle('Dataset Label Verification\\n(Top row: Non-Fractured, Bottom row: Fractured)', fontsize=14)\n",
        "\n",
        "    # Show 4 fractured and 4 non-fractured images\n",
        "    fractured_indices = [i for i, label in enumerate(train_labels) if label == 1]\n",
        "    non_fractured_indices = [i for i, label in enumerate(train_labels) if label == 0]\n",
        "\n",
        "    print(f\"Found {len(non_fractured_indices)} non-fractured images (label=0)\")\n",
        "    print(f\"Found {len(fractured_indices)} fractured images (label=1)\")\n",
        "\n",
        "    # Display NON-fractured images (label=0) in TOP row\n",
        "    for i in range(4):\n",
        "        if i < len(non_fractured_indices):\n",
        "            img_path = train_files[non_fractured_indices[i]]\n",
        "            img = Image.open(img_path)\n",
        "            axes[0, i].imshow(img, cmap='gray')\n",
        "            axes[0, i].set_title(f'NON-FRACTURED\\n(Label=0)', color='green')\n",
        "            axes[0, i].axis('off')\n",
        "        else:\n",
        "            axes[0, i].set_title('No Image')\n",
        "            axes[0, i].axis('off')\n",
        "\n",
        "    # Display FRACTURED images (label=1) in BOTTOM row  \n",
        "    for i in range(4):\n",
        "        if i < len(fractured_indices):\n",
        "            img_path = train_files[fractured_indices[i]]\n",
        "            img = Image.open(img_path)\n",
        "            axes[1, i].imshow(img, cmap='gray')\n",
        "            axes[1, i].set_title(f'FRACTURED\\n(Label=1)', color='red')\n",
        "            axes[1, i].axis('off')\n",
        "        else:\n",
        "            axes[1, i].set_title('No Image')\n",
        "            axes[1, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Check image sizes\n",
        "    sample_sizes = []\n",
        "    for i in range(min(10, len(train_files))):\n",
        "        img = Image.open(train_files[i])\n",
        "        sample_sizes.append(img.size)\n",
        "\n",
        "    print(f\"\\nSample image sizes: {sample_sizes[:5]}\")\n",
        "    print(f\"Unique sizes: {list(set(sample_sizes))}\")\n",
        "    \n",
        "    # VERIFICATION: Check if labels match folder names\n",
        "    print(f\"\\nLabel verification:\")\n",
        "    print(f\"Total images: {len(train_files)}\")\n",
        "    label_counts = pd.Series(train_labels).value_counts().sort_index()\n",
        "    print(f\"Label distribution:\")\n",
        "    for label, count in label_counts.items():\n",
        "        label_name = \"NON-FRACTURED\" if label == 0 else \"FRACTURED\"  \n",
        "        print(f\"  {label_name} (label={label}): {count} images\")\n",
        "    \n",
        "    # Show sample file paths to verify folder assignment\n",
        "    print(f\"\\nSample file paths with labels:\")\n",
        "    for i in range(min(5, len(train_files))):\n",
        "        label_name = \"NON-FRACTURED\" if train_labels[i] == 0 else \"FRACTURED\"\n",
        "        print(f\"  {os.path.basename(os.path.dirname(train_files[i]))} -> {label_name} (label={train_labels[i]})\")\n",
        "else:\n",
        "    print(\"No images found for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c833b60c",
      "metadata": {},
      "source": [
        "### Important: Label Assignment Verification\n",
        "\n",
        "If you've already trained models with potentially swapped labels, you have two options:\n",
        "\n",
        "1. **Retrain models** with corrected labels (recommended for accuracy)\n",
        "2. **Adjust prediction interpretation** in your Flask app (quick fix)\n",
        "\n",
        "The corrected labeling is:\n",
        "- **Label 0**: Non-Fractured  \n",
        "- **Label 1**: Fractured\n",
        "\n",
        "This matches your Flask app expectations: `fracture_classes = ['NON_FRACTURED', 'FRACTURED']`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b8adabb",
      "metadata": {
        "id": "1b8adabb"
      },
      "source": [
        "## 4. Custom Dataset Class and Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd8cc19d",
      "metadata": {
        "id": "bd8cc19d"
      },
      "outputs": [],
      "source": [
        "class FractureDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Load image\n",
        "            image_path = self.image_paths[idx]\n",
        "            image = cv2.imread(image_path)\n",
        "\n",
        "            if image is None:\n",
        "                # Try with PIL if cv2 fails\n",
        "                image = Image.open(image_path)\n",
        "                image = np.array(image)\n",
        "\n",
        "            # Convert BGR to RGB if needed\n",
        "            if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            elif len(image.shape) == 2:\n",
        "                # Convert grayscale to RGB\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "            # Apply transformations\n",
        "            if self.transform:\n",
        "                if isinstance(self.transform, A.Compose):\n",
        "                    augmented = self.transform(image=image)\n",
        "                    image = augmented['image']\n",
        "                else:\n",
        "                    # Convert to PIL for torchvision transforms\n",
        "                    image = Image.fromarray(image)\n",
        "                    image = self.transform(image)\n",
        "\n",
        "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "            return image, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            # Return a dummy image and label\n",
        "            dummy_image = torch.zeros(3, 224, 224)\n",
        "            dummy_label = torch.tensor(0, dtype=torch.long)\n",
        "            return dummy_image, dummy_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e81c02a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e81c02a",
        "outputId": "6e89a6b1-2735-4c79-e00a-de29843e436d"
      },
      "outputs": [],
      "source": [
        "# Define data augmentation and preprocessing\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Training transforms with augmentation\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=10, p=0.3),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Validation transforms (no augmentation)\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "print(\"Data transforms defined successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51c7d137",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51c7d137",
        "outputId": "1d70e262-245d-4dd7-ce78-c776783a2dce"
      },
      "outputs": [],
      "source": [
        "# Create datasets and data loaders using existing train/val/test splits\n",
        "if combined_files and 'train' in dataset_info:\n",
        "    print(\"\\n=== CREATING DATA LOADERS ===\")\n",
        "    \n",
        "    # Use existing train folder\n",
        "    if 'train' in dataset_info:\n",
        "        train_files = dataset_info['train']['files']\n",
        "        train_labels = dataset_info['train']['labels']\n",
        "        print(f\"Training data: {len(train_files)} images\")\n",
        "    \n",
        "    # Use existing val folder if available, otherwise create validation from train\n",
        "    if 'val' in dataset_info:\n",
        "        val_files = dataset_info['val']['files']\n",
        "        val_labels = dataset_info['val']['labels']\n",
        "        print(f\"Validation data: {len(val_files)} images (from existing val folder)\")\n",
        "    else:\n",
        "        # Fallback: split train data if no val folder exists\n",
        "        print(\"No validation folder found. Splitting training data...\")\n",
        "        train_files, val_files, train_labels, val_labels = train_test_split(\n",
        "            train_files, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
        "        )\n",
        "        print(f\"Training data (after split): {len(train_files)} images\")\n",
        "        print(f\"Validation data (from train split): {len(val_files)} images\")\n",
        "    \n",
        "    # Test data (optional - for final evaluation)\n",
        "    if 'test' in dataset_info:\n",
        "        test_files = dataset_info['test']['files']\n",
        "        test_labels = dataset_info['test']['labels']\n",
        "        print(f\"Test data: {len(test_files)} images (for final evaluation)\")\n",
        "    \n",
        "    # Print label distributions for each split\n",
        "    print(f\"\\nLabel distributions:\")\n",
        "    train_dist = pd.Series(train_labels).value_counts().sort_index()\n",
        "    val_dist = pd.Series(val_labels).value_counts().sort_index()\n",
        "    \n",
        "    print(f\"Training:\")\n",
        "    for label, count in train_dist.items():\n",
        "        label_name = \"NON-FRACTURED\" if label == 0 else \"FRACTURED\"\n",
        "        print(f\"  {label_name}: {count} ({count/len(train_labels)*100:.1f}%)\")\n",
        "    \n",
        "    print(f\"Validation:\")\n",
        "    for label, count in val_dist.items():\n",
        "        label_name = \"NON-FRACTURED\" if label == 0 else \"FRACTURED\"\n",
        "        print(f\"  {label_name}: {count} ({count/len(val_labels)*100:.1f}%)\")\n",
        "    \n",
        "    if 'test' in dataset_info:\n",
        "        test_dist = pd.Series(test_labels).value_counts().sort_index()\n",
        "        print(f\"Test:\")\n",
        "        for label, count in test_dist.items():\n",
        "            label_name = \"NON-FRACTURED\" if label == 0 else \"FRACTURED\"\n",
        "            print(f\"  {label_name}: {count} ({count/len(test_labels)*100:.1f}%)\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = FractureDataset(train_files, train_labels, transform=train_transform)\n",
        "    val_dataset = FractureDataset(val_files, val_labels, transform=val_transform)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"\\nData loaders created:\")\n",
        "    print(f\"  Train batches: {len(train_loader)} (batch size: {BATCH_SIZE})\")\n",
        "    print(f\"  Validation batches: {len(val_loader)} (batch size: {BATCH_SIZE})\")\n",
        "    \n",
        "    # Optional: Create test loader if test data exists\n",
        "    if 'test' in dataset_info:\n",
        "        test_dataset = FractureDataset(test_files, test_labels, transform=val_transform)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "        print(f\"  Test batches: {len(test_loader)} (batch size: {BATCH_SIZE})\")\n",
        "    \n",
        "    # Test loading a batch\n",
        "    try:\n",
        "        sample_batch = next(iter(train_loader))\n",
        "        print(f\"\\nSample batch verification:\")\n",
        "        print(f\"  Batch shape: {sample_batch[0].shape}\")\n",
        "        print(f\"  Labels shape: {sample_batch[1].shape}\")\n",
        "        print(f\"  Sample labels: {sample_batch[1][:8].tolist()}\")  # Show first 8 labels\n",
        "        print(\"✅ Data loaders created successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in data loading: {e}\")\n",
        "else:\n",
        "    print(\"❌ No training files found. Please check the dataset paths.\")\n",
        "    train_loader = None\n",
        "    val_loader = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc32d277",
      "metadata": {
        "id": "fc32d277"
      },
      "source": [
        "## 5. Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38010f43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38010f43",
        "outputId": "381404c2-c3c6-4f32-bd9c-b060a7b4d516"
      },
      "outputs": [],
      "source": [
        "class FractureResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(FractureResNet50, self).__init__()\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Freeze early layers\n",
        "        for param in list(self.backbone.parameters())[:-20]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace the final layer\n",
        "        self.backbone.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(self.backbone.fc.in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "class FractureDenseNet121(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(FractureDenseNet121, self).__init__()\n",
        "        self.backbone = models.densenet121(pretrained=True)\n",
        "\n",
        "        # Freeze early layers\n",
        "        for param in list(self.backbone.parameters())[:-20]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace the final layer\n",
        "        self.backbone.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(self.backbone.classifier.in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "class FractureEfficientNetB0(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(FractureEfficientNetB0, self).__init__()\n",
        "        self.backbone = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "        # Freeze early layers\n",
        "        for param in list(self.backbone.parameters())[:-20]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace the final layer\n",
        "        self.backbone._fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(self.backbone._fc.in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "print(\"Model classes defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de2a152",
      "metadata": {
        "id": "7de2a152"
      },
      "source": [
        "## 6. Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da009829",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da009829",
        "outputId": "6a6c211f-1a4f-49ba-a9c9-3ca037e1679b"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Train a model and return training history\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        print(f'Best Val Acc: {best_val_acc:.2f}%')\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    \"\"\"\n",
        "    Evaluate model and return predictions for detailed analysis\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    return all_predictions, all_labels, all_probabilities\n",
        "\n",
        "print(\"Training functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46946d57",
      "metadata": {
        "id": "46946d57"
      },
      "source": [
        "## 7. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad4231d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ad4231d",
        "outputId": "e925f7f5-1701-4b1b-88bc-0129ffb8307f"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "NUM_EPOCHS = 15\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Initialize models\n",
        "models_dict = {\n",
        "    'ResNet50': FractureResNet50(),\n",
        "    'DenseNet121': FractureDenseNet121(),\n",
        "    'EfficientNetB0': FractureEfficientNetB0()\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "training_histories = {}\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "print(f\"Training on device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c39a9a7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c39a9a7d",
        "outputId": "6251f5be-bfd7-4625-d4c7-b42d2cb16aad"
      },
      "outputs": [],
      "source": [
        "# Train ResNet50\n",
        "if train_files:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING RESNET50\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    resnet_model, resnet_history = train_model(\n",
        "        models_dict['ResNet50'],\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE\n",
        "    )\n",
        "\n",
        "    trained_models['ResNet50'] = resnet_model\n",
        "    training_histories['ResNet50'] = resnet_history\n",
        "\n",
        "    # Save model to Google Drive\n",
        "    if IN_COLAB:\n",
        "        model_path = f'{models_dir}/fracture_resnet50.pth'\n",
        "    else:\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        model_path = 'models/fracture_resnet50.pth'\n",
        "\n",
        "    torch.save(resnet_model.state_dict(), model_path)\n",
        "    print(f\"ResNet50 model saved to: {model_path}\")\n",
        "else:\n",
        "    print(\"Skipping training - no data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c200e9c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c200e9c4",
        "outputId": "db0e6ad7-5459-4622-8e84-d34c476e31ed"
      },
      "outputs": [],
      "source": [
        "# Train DenseNet121\n",
        "if train_files:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING DENSENET121\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    densenet_model, densenet_history = train_model(\n",
        "        models_dict['DenseNet121'],\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE\n",
        "    )\n",
        "\n",
        "    trained_models['DenseNet121'] = densenet_model\n",
        "    training_histories['DenseNet121'] = densenet_history\n",
        "\n",
        "    # Save model to Google Drive\n",
        "    if IN_COLAB:\n",
        "        model_path = f'{models_dir}/fracture_densenet121.pth'\n",
        "    else:\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        model_path = 'models/fracture_densenet121.pth'\n",
        "\n",
        "    torch.save(densenet_model.state_dict(), model_path)\n",
        "    print(f\"DenseNet121 model saved to: {model_path}\")\n",
        "else:\n",
        "    print(\"Skipping training - no data available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a22c7b17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a22c7b17",
        "outputId": "4f065bb2-af0d-49c2-dfca-c2e25f438c2c"
      },
      "outputs": [],
      "source": [
        "# Train EfficientNetB0\n",
        "if train_files:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING EFFICIENTNETB0\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    efficientnet_model, efficientnet_history = train_model(\n",
        "        models_dict['EfficientNetB0'],\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE\n",
        "    )\n",
        "\n",
        "    trained_models['EfficientNetB0'] = efficientnet_model\n",
        "    training_histories['EfficientNetB0'] = efficientnet_history\n",
        "\n",
        "    # Save model to Google Drive\n",
        "    if IN_COLAB:\n",
        "        model_path = f'{models_dir}/fracture_efficientnetb0.pth'\n",
        "    else:\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        model_path = 'models/fracture_efficientnetb0.pth'\n",
        "\n",
        "    torch.save(efficientnet_model.state_dict(), model_path)\n",
        "    print(f\"EfficientNetB0 model saved to: {model_path}\")\n",
        "else:\n",
        "    print(\"Skipping training - no data available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c080cd5",
      "metadata": {
        "id": "6c080cd5"
      },
      "source": [
        "## 8. Results Visualization and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a74778d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "3a74778d",
        "outputId": "0144639a-b68f-4a1e-9c93-83b329650448"
      },
      "outputs": [],
      "source": [
        "# Plot training histories\n",
        "if training_histories:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Training Loss\n",
        "    for model_name, history in training_histories.items():\n",
        "        axes[0, 0].plot(history['train_loss'], label=f'{model_name} Train')\n",
        "    axes[0, 0].set_title('Training Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    # Validation Loss\n",
        "    for model_name, history in training_histories.items():\n",
        "        axes[0, 1].plot(history['val_loss'], label=f'{model_name} Val')\n",
        "    axes[0, 1].set_title('Validation Loss')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # Training Accuracy\n",
        "    for model_name, history in training_histories.items():\n",
        "        axes[1, 0].plot(history['train_acc'], label=f'{model_name} Train')\n",
        "    axes[1, 0].set_title('Training Accuracy')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    # Validation Accuracy\n",
        "    for model_name, history in training_histories.items():\n",
        "        axes[1, 1].plot(history['val_acc'], label=f'{model_name} Val')\n",
        "    axes[1, 1].set_title('Validation Accuracy')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No training histories to plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b7d60f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b7d60f",
        "outputId": "f6978ad7-4770-4f6f-bbe0-49a71dde0dc4"
      },
      "outputs": [],
      "source": [
        "# Evaluate all models and compare performance\n",
        "if trained_models and val_loader:\n",
        "    model_results = {}\n",
        "\n",
        "    for model_name, model in trained_models.items():\n",
        "        print(f\"\\nEvaluating {model_name}...\")\n",
        "        predictions, true_labels, probabilities = evaluate_model(model, val_loader)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "        model_results[model_name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'predictions': predictions,\n",
        "            'true_labels': true_labels,\n",
        "            'probabilities': probabilities\n",
        "        }\n",
        "\n",
        "        print(f\"{model_name} - Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"{model_name} - Classification Report:\")\n",
        "        print(classification_report(true_labels, predictions,\n",
        "                                  target_names=['Non-Fractured', 'Fractured']))\n",
        "\n",
        "    # Summary comparison\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MODEL COMPARISON SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for model_name, results in model_results.items():\n",
        "        print(f\"{model_name}: {results['accuracy']:.4f}\")\n",
        "\n",
        "    # Find best model\n",
        "    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])\n",
        "    best_accuracy = model_results[best_model_name]['accuracy']\n",
        "\n",
        "    print(f\"\\nBest Model: {best_model_name} (Accuracy: {best_accuracy:.4f})\")\n",
        "else:\n",
        "    print(\"No trained models to evaluate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e80eb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "33e80eb1",
        "outputId": "c1c89735-5827-4b10-8c33-94e81a8dc318"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrices\n",
        "if 'model_results' in locals():\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    for idx, (model_name, results) in enumerate(model_results.items()):\n",
        "        cm = confusion_matrix(results['true_labels'], results['predictions'])\n",
        "\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Non-Fractured', 'Fractured'],\n",
        "                   yticklabels=['Non-Fractured', 'Fractured'],\n",
        "                   ax=axes[idx])\n",
        "\n",
        "        axes[idx].set_title(f'{model_name}\\nAccuracy: {results[\"accuracy\"]:.3f}')\n",
        "        axes[idx].set_xlabel('Predicted')\n",
        "        axes[idx].set_ylabel('Actual')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6b7a54",
      "metadata": {
        "id": "1d6b7a54"
      },
      "source": [
        "## 9. Model Export and Download (For Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a04b372",
      "metadata": {
        "id": "2a04b372"
      },
      "outputs": [],
      "source": [
        "# Save all trained models and create a summary in Google Drive\n",
        "if trained_models:\n",
        "    # Create a summary of results\n",
        "    results_summary = {\n",
        "        'training_parameters': {\n",
        "            'epochs': NUM_EPOCHS,\n",
        "            'learning_rate': LEARNING_RATE,\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'image_size': IMG_SIZE\n",
        "        },\n",
        "        'model_performance': {},\n",
        "        'storage_info': {\n",
        "            'dataset_location': dataset_dir if IN_COLAB else './dataset',\n",
        "            'models_location': models_dir if IN_COLAB else './models'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if 'model_results' in locals():\n",
        "        for model_name, results in model_results.items():\n",
        "            results_summary['model_performance'][model_name] = {\n",
        "                'accuracy': float(results['accuracy']),\n",
        "                'best_val_accuracy': max(training_histories[model_name]['val_acc'])\n",
        "            }\n",
        "\n",
        "    # Save summary to Google Drive\n",
        "    import json\n",
        "    if IN_COLAB:\n",
        "        summary_path = f'{models_dir}/training_summary.json'\n",
        "    else:\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        summary_path = 'models/training_summary.json'\n",
        "\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(results_summary, f, indent=2)\n",
        "\n",
        "    print(f\"Training summary saved to: {summary_path}\")\n",
        "\n",
        "    # List all saved files\n",
        "    print(\"\\nSaved files in Google Drive:\" if IN_COLAB else \"\\nSaved files:\")\n",
        "\n",
        "    if IN_COLAB:\n",
        "        saved_files = [\n",
        "            f'{models_dir}/fracture_resnet50.pth',\n",
        "            f'{models_dir}/fracture_densenet121.pth',\n",
        "            f'{models_dir}/fracture_efficientnetb0.pth',\n",
        "            f'{models_dir}/training_summary.json'\n",
        "        ]\n",
        "    else:\n",
        "        saved_files = [\n",
        "            'models/fracture_resnet50.pth',\n",
        "            'models/fracture_densenet121.pth',\n",
        "            'models/fracture_efficientnetb0.pth',\n",
        "            'models/training_summary.json'\n",
        "        ]\n",
        "\n",
        "    for file in saved_files:\n",
        "        if os.path.exists(file):\n",
        "            size = os.path.getsize(file) / (1024*1024)  # Size in MB\n",
        "            print(f\"- {os.path.basename(file)} ({size:.1f} MB)\")\n",
        "            print(f\"  Full path: {file}\")\n",
        "\n",
        "    # Optional: Also create local copies for download if on Colab\n",
        "    if IN_COLAB:\n",
        "        print(\"\\nCreating local copies for download...\")\n",
        "        from google.colab import files\n",
        "\n",
        "        local_files = []\n",
        "        for file in saved_files:\n",
        "            if os.path.exists(file):\n",
        "                local_name = os.path.basename(file)\n",
        "                # Copy to local content directory\n",
        "                import shutil\n",
        "                shutil.copy2(file, f'/content/{local_name}')\n",
        "                local_files.append(local_name)\n",
        "                print(f\"Local copy created: /content/{local_name}\")\n",
        "\n",
        "        # Ask user if they want to download\n",
        "        download_choice = input(\"\\nDo you want to download model files locally? (y/n): \").lower()\n",
        "        if download_choice == 'y':\n",
        "            for local_file in local_files:\n",
        "                try:\n",
        "                    files.download(local_file)\n",
        "                    print(f\"Downloaded: {local_file}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error downloading {local_file}: {e}\")\n",
        "\n",
        "    print(f\"\\n✅ All models and dataset are now saved in Google Drive!\")\n",
        "    if IN_COLAB:\n",
        "        print(f\"📁 Dataset: {dataset_dir}\")\n",
        "        print(f\"🤖 Models: {models_dir}\")\n",
        "        print(f\"\\nYour data will persist across Colab sessions!\")\n",
        "\n",
        "else:\n",
        "    print(\"No trained models to save\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55dae9a3",
      "metadata": {
        "id": "55dae9a3"
      },
      "source": [
        "## 10. Model Loading Template (For Future Use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ee21c8",
      "metadata": {
        "id": "59ee21c8"
      },
      "outputs": [],
      "source": [
        "# Template code for loading trained models from Google Drive in your Flask app\n",
        "template_code = \"\"\"\n",
        "# Template code to load the trained fracture detection models\n",
        "# Copy this to your Flask application\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# Model definitions (copy from above)\n",
        "class FractureResNet50(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(FractureResNet50, self).__init__()\n",
        "        self.backbone = models.resnet50(pretrained=False)\n",
        "        self.backbone.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "# Similar class definitions for DenseNet121 and EfficientNetB0...\n",
        "\n",
        "# Loading the models from Google Drive (when running in Colab)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# For Google Colab - mount drive first:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Model paths (update these paths based on your Google Drive structure)\n",
        "MODEL_BASE_PATH = '/content/drive/MyDrive/fracture_detection/models'\n",
        "# For local use: MODEL_BASE_PATH = './models'\n",
        "\n",
        "# Load ResNet50\n",
        "resnet_model = FractureResNet50()\n",
        "resnet_path = f'{MODEL_BASE_PATH}/fracture_resnet50.pth'\n",
        "resnet_model.load_state_dict(torch.load(resnet_path, map_location=device))\n",
        "resnet_model.eval()\n",
        "\n",
        "# Load DenseNet121\n",
        "densenet_model = FractureDenseNet121()\n",
        "densenet_path = f'{MODEL_BASE_PATH}/fracture_densenet121.pth'\n",
        "densenet_model.load_state_dict(torch.load(densenet_path, map_location=device))\n",
        "densenet_model.eval()\n",
        "\n",
        "# Load EfficientNetB0\n",
        "efficientnet_model = FractureEfficientNetB0()\n",
        "efficientnet_path = f'{MODEL_BASE_PATH}/fracture_efficientnetb0.pth'\n",
        "efficientnet_model.load_state_dict(torch.load(efficientnet_path, map_location=device))\n",
        "efficientnet_model.eval()\n",
        "\n",
        "# Prediction function\n",
        "def predict_fracture(image, model):\n",
        "    '''\n",
        "    Predict fracture from preprocessed image tensor\n",
        "    Returns: (predicted_class, confidence_score)\n",
        "    - predicted_class: 0 = Non-Fractured, 1 = Fractured\n",
        "    - confidence_score: float between 0-1\n",
        "    '''\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        predicted = torch.argmax(outputs, 1)\n",
        "        confidence = torch.max(probabilities, 1)[0]\n",
        "\n",
        "    return predicted.item(), confidence.item()\n",
        "\n",
        "# Example usage in Flask app:\n",
        "# prediction, confidence = predict_fracture(preprocessed_image, resnet_model)\n",
        "# result = \"Fractured\" if prediction == 1 else \"Non-Fractured\"\n",
        "\"\"\"\n",
        "\n",
        "print(\"Model loading template for Google Drive:\")\n",
        "print(template_code)\n",
        "\n",
        "# Also save this template to Google Drive\n",
        "if IN_COLAB:\n",
        "    template_path = f'{models_dir}/flask_integration_template.py'\n",
        "else:\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    template_path = 'models/flask_integration_template.py'\n",
        "\n",
        "with open(template_path, 'w') as f:\n",
        "    f.write(template_code)\n",
        "\n",
        "print(f\"\\n📄 Template saved to: {template_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d712cb",
      "metadata": {
        "id": "b2d712cb"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has successfully:\n",
        "\n",
        "1. **Setup Environment**: Configured Google Colab with required dependencies and Google Drive integration\n",
        "2. **Data Management**: Downloaded and stored the Kaggle fracture dataset in Google Drive for persistence\n",
        "3. **Model Architecture**: Implemented three state-of-the-art models:\n",
        "   - ResNet50 with custom classifier\n",
        "   - DenseNet121 with custom classifier  \n",
        "   - EfficientNetB0 with custom classifier\n",
        "4. **Training**: Trained all models with proper augmentation and validation\n",
        "5. **Evaluation**: Compared model performance with detailed metrics\n",
        "6. **Persistent Storage**: Saved trained models to Google Drive `/models` folder for future use\n",
        "\n",
        "## 📁 Google Drive Structure Created:\n",
        "\n",
        "```\n",
        "/content/drive/MyDrive/fracture_detection/\n",
        "├── dataset/                          # Kaggle fracture dataset\n",
        "│   ├── fracture-multi-region-x-ray-data.zip\n",
        "│   └── [extracted dataset files]\n",
        "└── models/                           # Trained models and outputs\n",
        "    ├── fracture_resnet50.pth         # ResNet50 model weights\n",
        "    ├── fracture_densenet121.pth      # DenseNet121 model weights  \n",
        "    ├── fracture_efficientnetb0.pth   # EfficientNetB0 model weights\n",
        "    ├── training_summary.json         # Training metrics and info\n",
        "    └── flask_integration_template.py # Code template for Flask app\n",
        "```\n",
        "\n",
        "## 🚀 Key Benefits:\n",
        "\n",
        "- **Persistent Storage**: Dataset and models survive Colab session restarts\n",
        "- **Easy Access**: Models accessible from any Colab notebook via Drive mount\n",
        "- **Organized Structure**: Clean folder organization for easy management\n",
        "- **Integration Ready**: Template code provided for Flask app integration\n",
        "\n",
        "## 📋 Next Steps:\n",
        "\n",
        "1. **Access Models**: Mount Google Drive in any future Colab session to access trained models\n",
        "2. **Flask Integration**: Use the provided template to load models in your web application  \n",
        "3. **Model Deployment**: Download models from Drive for production deployment\n",
        "4. **Continuous Training**: Easily retrain models using the persistent dataset\n",
        "\n",
        "The trained models can now be integrated into your medical imaging application for fracture detection. Each model provides binary classification (Fractured vs Non-Fractured) with confidence scores, and all data is safely stored in your Google Drive!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
